{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit729ca97937d141b791e090f75bd8ee6a",
   "display_name": "Python 3.8.2 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ENSTA-ParisTech 3e année - Cours ROB313\n",
    "# TP4: Analyse vidéo et Tracking\n",
    "\n",
    "**Auteurs: Kevin Alessandro Sanchez Diaz et Yevhenii Sielskyi**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Mean Shift\n",
    "\n",
    "**Q1 Expérimenter** le suivi réalisé par le code de base `Tracking_MeanShift.py` fourni qui utilise l'algorithme de *Mean Shift*, avec la densité marginale $f_H$ sur la composante $H$ de teinte. Rappeler le principe de l'algorithme *Mean Shift*, et illustrer par vos expériences ses avantages et ses limitations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Tracking_MeanShift.py"
   ]
  },
  {
   "source": [
    "> Nous avons testé l'implémentation donnée de l'algorithme de *Mean Shift* sur $5$ vidéos de tests différents dont les résultats sont suivants:\n",
    "\n",
    "> *Mean Shift* en tant qu'algorithme simple de l'estimation de la mode d'une distribution, souffre de la similarité des couleurs entre l'objet suivi et le fond, les autres objets perturbant les cadres analysés. Sur la **figure 1** nous voyons le suivi d'une tasse dont la couleur est grise claire, qui ressemble à celle de mur en arrière-plan. Dès le début du suivi, le rectangle bleu se place sur la région de mur mentionné, si la taille de *RDI* (*R*égion *D*'*I*ntérêt) choisie n'est pas suffisamment grande.\n",
    "\n",
    "**Figure 1 &mdash; suivi de la tasse avec *Mean Shift***\n",
    "\n",
    "![q1_mug](images/q1_mug.png)\n",
    "\n",
    "> Ceci est dû au fait que l'indice de Bhattacharyya est utilisé en tant que mesure de similarité entre la distribution locale actuelle et celle de référence: les petits changements de la teinte (ce qui correspond au canal *H* des images HSV, utilisé pour *Backproject*) d'objet peuvent aboutir à sa perte par RDI. *Figure 2* représente les résultats du suivi du visage d'un jeune homme, qui est \"disparu\" en entrant dans l'ombre.\n",
    "\n",
    "**Figure 2 &mdash; suivi d'un visage dans les zones à l'éclairage contrasté**\n",
    "\n",
    "![q1_sunshade](images/q1_sunshade.png)\n",
    "\n",
    "> Prenons maintenant l'exemple de suivi d'un objet qui se distingue par ses couleurs dans son environnement. Le ballon rouge ci-dessous est unique dans son genre sur le tapis et avec les meubles de couleurs neutres autour. Mais dans ce cas-là, nous faisons face à un autre inconvénient de *Mean Shift* &mdash; les changements rapides de la position d'objet relative à RDI: **figure 3** répresente le cadre de perte du ballon du focus lors de son déplacement et du mouvement brusque de la caméra en même temps. Après avoir perdu le ballon et s'en être éloigné considérablement, RDI reste fixé sur le tapis. L'objet de suivi est recapturé lors de son passage à côté de rectangle bleu (voir **figure 4**).\n",
    "\n",
    "**Figure 3 &mdash; suivi du ballon rouge avec *Mean Shift*. Ballon perdu**\n",
    "\n",
    "![q1_ball](images/q1_ball.png)\n",
    "\n",
    "**Figure 4 &mdash; suivi du ballon rouge avec *Mean Shift*. Ballon retrouvé**\n",
    "\n",
    "![q1_ball2](images/q1_ball2.png)\n",
    "\n",
    "> Le même est obtenu dans le cadre du suivi d'une voiture (voir *figure 5*). Notons ici l'instabilité de la caméra ainsi que les couleurs neutres de la voiture (les phares ne sont pas inclus dans le ROI initial &mdash; rectangle vert).\n",
    "\n",
    "**Figure 5 &mdash; suivi de la voiture avec *Mean Shift***\n",
    "\n",
    "![q1_car](images/q1_car.png)\n",
    "\n",
    "> Encore une cause possible du changement rapide des distributions locales &mdash; zoom des vidéos. Les *figures 6* et *7* démontrent la démarche de *Mean Shift* avant et après la mise à l'échelle d'enregistrement de la promenade d'une femme.\n",
    "\n",
    "**Figure 6 &mdash; suivi d'une femme avec *Mean Shift*. Focus lointain**\n",
    "\n",
    "![q1_woman](images/q1_woman.png)\n",
    "\n",
    "**Figure 7 &mdash; suivi d'une femme avec *Mean Shift*. Focus mis à l'échelle**\n",
    "\n",
    "![q1_woman2](images/q1_woman2.png)\n",
    "\n",
    "> En revanche, la complexité et le temps de calculs de *Mean Shift* sont petits ce qui rend cette méthode acceptable pour les problèmes simples (objet clair sur le fond neutre, etc.) de suivi d'objets en temps réel, même si l'algorithme est sensible au choix de la RDI. Nous verrons par la suite, que la Transformée de Hough exige beaucoup plus de temps pour calculer chaque cadre avec le nombre important de points significatifs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Q2 Analyser** plus finement le résultat en affichant la séquence des poids à partir de la rétroprojection $R_H$ de l'histogramme $f_H$ de teinte, définie par $R_H(x, y) = f_H(H(x, y))$. Proposer et programmer des améliorations, en changeant la densité calculée et/ou en mettant en oeuvre une stratégie de mise à jour de l'histogramme modèle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> TODO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Transformée de Hough\n",
    "\n",
    "**Q3 Calculer** à chaque trame, l'orientation locale, i.e. l'argument du gradient des pixels de l'image, ainsi que le module du gradient. Définir un seuil sur le module du gradient pour masquer les pixels dont l'orientation n'est pas significative. Afficher ainsi la séquence des orientations où les pixels masqués apparaissent en rouge. L'objectif de cette question est de définir l'index de la *Transformée de Hough* (l'orientation), ainsi que l'ensemble des pixels votants, i.e. ceux dont l'orientation est significative."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Le code performant des transformations décrites ci-dessus se trouve dans le script `voting_pixels.py`. Les résultats de ses calculs sont présentés sur la *figure 8*.\n",
    "\n",
    "**Figure 8 &mdash; orientation de gradient, sa norme et l'orientation masquée pour le cadre de tasse**\n",
    "\n",
    "![q3_mug](images/q3_mug.png)\n",
    "\n",
    "> Le script lui-même peut être testé avec la commande suivante: `python voting_pixels.py`, qui permet d'obtenir les images (voir *figure 9*) similaires aux exemples donnés dans l'énoncé.\n",
    "\n",
    "**Figure 9 &mdash; orientation de gradient, sa norme et l'orientation masquée pour le cadre de ballon**\n",
    "\n",
    "![q3_ball](images/q3_ball.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Q4 Construire** un modèle de l'objet défini initialement sous la forme d'un modèle implicite indexé sur l'orientation (*R-Table*). Puis calculer la transformée de Hough associée sur toutes les images de la séquence. Calculer le suivi correspondant à la valeur maximale de la transformée de Hough à chaque image. Commenter et critiquer le résultat obtenu."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> TODO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Q5 Remplacer** le calcul du maximum par l'application du Mean Shift sur la transformée de Hough. Interpréter le résultat et le comparer avec le précédent. Proposer une stratégie de mise à jour du modèle qui permette de prendre en compte les déformations de l'objet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> TODO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}